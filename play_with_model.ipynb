{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Play_lstm_seq_vae.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "akHUDpl5RRWk",
        "outputId": "e91453fb-81c2-42c5-9046-645e7ef798c9"
      },
      "source": [
        "#@title Download data and files.\n",
        "!git clone https://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@github.com/Khamies/LSTM-Sequence-VAE.git\n",
        "import os \n",
        "os.chdir(\"LSTM-Sequence-VAE\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LSTM-Sequence-VAE'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 138 (delta 62), reused 102 (delta 31), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (138/138), 29.95 MiB | 22.90 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "data\t loss.py  media     models   Pipfile.lock  README.md\ttrain.py\n",
            "LICENSE  main.py  model.py  Pipfile  play.py\t   settings.py\tutils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsX47UMWT0NT"
      },
      "source": [
        "import torch\n",
        "from data.ptb import PTB\n",
        "from model import LSTM_VAE"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6QyBIaJWGLI"
      },
      "source": [
        "# Settings\n",
        "\n",
        "torch.manual_seed(1000)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 32\n",
        "bptt = 60\n",
        "lr = 0.001\n",
        "\n",
        "embed_size = 300\n",
        "hidden_size = 256\n",
        "latent_size = 16\n",
        "lstm_layer=1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcXo1SRDfIVm"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2uVseOtT_4J"
      },
      "source": [
        "# Load the data\n",
        "train_data = PTB(data_dir=\"./data\", split=\"train\", create_data= False, max_sequence_length= bptt)\n",
        "test_data = PTB(data_dir=\"./data\", split=\"test\", create_data= False, max_sequence_length= bptt)\n",
        "valid_data = PTB(data_dir=\"./data\", split=\"valid\", create_data= False, max_sequence_length= bptt)\n",
        "\n",
        "# Batchify the data\n",
        "train_loader = torch.utils.data.DataLoader( dataset= train_data, batch_size= batch_size, shuffle= True, pin_memory=torch.cuda.is_available())\n",
        "test_loader = torch.utils.data.DataLoader( dataset= test_data, batch_size= batch_size, shuffle= True, pin_memory=torch.cuda.is_available())\n",
        "valid_loader = torch.utils.data.DataLoader( dataset= valid_data, batch_size= batch_size, shuffle= True, pin_memory=torch.cuda.is_available())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-waTAMaZUIY9"
      },
      "source": [
        "def interpolate(model, n_interpolations, sos, sequence_length):\n",
        "\n",
        "  # # Get input.\n",
        "\n",
        "  z1 = torch.randn((1,1,latent_size)).to(device)\n",
        "  z2 = torch.randn((1,1,latent_size)).to(device)\n",
        "\n",
        "  text1 = model.inference(sequence_length , sos, z1)\n",
        "  text2 = model.inference(sequence_length , sos, z2)\n",
        "\n",
        "  alpha_s = torch.linspace(0,1,n_interpolations)\n",
        "\n",
        "  interpolations = torch.stack([alpha*z1 + (1-alpha)*z2  for alpha in alpha_s])\n",
        "\n",
        "\n",
        "  samples = [model.inference(sequence_length , sos, z) for z in interpolations]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return samples, text1, text2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ2jOq3se8-x"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw2_Fc_9Vhh5",
        "outputId": "0ffd9219-3097-4140-f899-b1d982c063d0"
      },
      "source": [
        "vocab_size = train_data.vocab_size\n",
        "model = LSTM_VAE(vocab_size = vocab_size, embed_size = embed_size, hidden_size = hidden_size, latent_size = latent_size).to(device)\n",
        "\n",
        "checkpoint = torch.load(\"models/LSTM_VAE.pt\")\n",
        "model.load_state_dict(checkpoint[\"model\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKhUHL_7Vg5X",
        "outputId": "cbd2db6b-67ab-4678-b0ea-4cfdc3f56069"
      },
      "source": [
        "#@title Sample Generation\n",
        "# inference\n",
        "z1 = torch.randn(1,1,latent_size).to(device)\n",
        "z2 = torch.randn(1,1,latent_size).to(device)\n",
        "\n",
        "sos = \"<sos>\"\n",
        "sample1 = model.inference(10 , sos, z1)\n",
        "sample2 = model.inference(10 , sos , z2)\n",
        "\n",
        "\n",
        "print(sample1)\n",
        "print(sample2)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the company said it will close the <unk> offering for\n",
            "the company said it will close the <unk> offering for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eme4eXbjVPOM",
        "outputId": "0db5e4c4-3e49-452c-cd97-514519133f30"
      },
      "source": [
        "#@title Interpolation\n",
        "samples, text1, text2 = interpolate(model, 20,\"president\", 10)\n",
        "print(\"First sentence:\", text1)\n",
        "print(\"Second sentence:\", text2)\n",
        "\n",
        "for sample in samples: print(sample)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First sentence: bush veto power would require such a technical crisis if\n",
            "Second sentence: bush veto power he declared the notes are being redeemed\n",
            "bush veto power he declared the notes are being redeemed\n",
            "bush veto power he declared the <unk> <eos> <eos> <unk>\n",
            "bush veto power he declared the <unk> <eos> <eos> <unk>\n",
            "bush veto power he declared the <unk> <eos> <eos> <unk>\n",
            "bush veto power he declared the <unk> <eos> <eos> <unk>\n",
            "bush veto power would require the fed to take advantage\n",
            "bush veto power would require the fed to take advantage\n",
            "bush veto power would require the fed to take advantage\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require such a technical crisis as\n",
            "bush veto power would require such a technical crisis as\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n"
          ]
        }
      ]
    }
  ]
}