{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/khamies//LSTM-Sequence-VAE/blob/master/play_with_model.ipynb\" \n",
        "target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#@title Download data and files.\n",
        "!git clone https://github.com/Khamies/LSTM-Variational-AutoEncoder.git\n",
        "import os \n",
        "os.chdir(\"LSTM-Variational-AutoEncoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bsX47UMWT0NT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from data.ptb import PTB\n",
        "from model import LSTM_VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u6QyBIaJWGLI"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "\n",
        "torch.manual_seed(1000)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "batch_size = 32\n",
        "bptt = 60\n",
        "lr = 0.001\n",
        "\n",
        "embed_size = 300\n",
        "hidden_size = 256\n",
        "latent_size = 16\n",
        "lstm_layer=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcXo1SRDfIVm"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "k2uVseOtT_4J"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "train_data = PTB(data_dir=\"./data\", split=\"train\", create_data= False, max_sequence_length= bptt)\n",
        "test_data = PTB(data_dir=\"./data\", split=\"test\", create_data= False, max_sequence_length= bptt)\n",
        "valid_data = PTB(data_dir=\"./data\", split=\"valid\", create_data= False, max_sequence_length= bptt)\n",
        "\n",
        "# Batchify the data\n",
        "train_loader = torch.utils.data.DataLoader( dataset= train_data, batch_size= batch_size, shuffle= True, pin_memory=torch.cuda.is_available())\n",
        "test_loader = torch.utils.data.DataLoader( dataset= test_data, batch_size= batch_size, shuffle= True, pin_memory=torch.cuda.is_available())\n",
        "valid_loader = torch.utils.data.DataLoader( dataset= valid_data, batch_size= batch_size, shuffle= True, pin_memory=torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-waTAMaZUIY9"
      },
      "outputs": [],
      "source": [
        "def interpolate(model, n_interpolations, sos, sequence_length):\n",
        "\n",
        "  # # Get input.\n",
        "\n",
        "  z1 = torch.randn((1,1,latent_size)).to(device)\n",
        "  z2 = torch.randn((1,1,latent_size)).to(device)\n",
        "\n",
        "  text1 = model.inference(sequence_length , sos, z1)\n",
        "  text2 = model.inference(sequence_length , sos, z2)\n",
        "\n",
        "  alpha_s = torch.linspace(0,1,n_interpolations)\n",
        "\n",
        "  interpolations = torch.stack([alpha*z1 + (1-alpha)*z2  for alpha in alpha_s])\n",
        "\n",
        "\n",
        "  samples = [model.inference(sequence_length , sos, z) for z in interpolations]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return samples, text1, text2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ2jOq3se8-x"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw2_Fc_9Vhh5",
        "outputId": "0ffd9219-3097-4140-f899-b1d982c063d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = train_data.vocab_size\n",
        "model = LSTM_VAE(vocab_size = vocab_size, embed_size = embed_size, hidden_size = hidden_size, latent_size = latent_size).to(device)\n",
        "\n",
        "checkpoint = torch.load(\"models/LSTM_VAE.pt\")\n",
        "model.load_state_dict(checkpoint[\"model\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKhUHL_7Vg5X",
        "outputId": "cbd2db6b-67ab-4678-b0ea-4cfdc3f56069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the company said it will close the <unk> offering for\n",
            "the company said it will close the <unk> offering for\n"
          ]
        }
      ],
      "source": [
        "#@title Sample Generation\n",
        "# inference\n",
        "z1 = torch.randn(1,1,latent_size).to(device)\n",
        "z2 = torch.randn(1,1,latent_size).to(device)\n",
        "\n",
        "sos = \"<sos>\"\n",
        "sample1 = model.inference(10 , sos, z1)\n",
        "sample2 = model.inference(10 , sos , z2)\n",
        "\n",
        "\n",
        "print(sample1)\n",
        "print(sample2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eme4eXbjVPOM",
        "outputId": "0db5e4c4-3e49-452c-cd97-514519133f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First sentence: bush veto power would require such a technical crisis if\n",
            "Second sentence: bush veto power he declared the notes are being redeemed\n",
            "bush veto power he declared the notes are being redeemed\n",
            "bush veto power he declared the <unk> <eos> <eos> <unk>\n",
            "bush veto power he declared the <unk> <eos> <eos> <unk>\n",
            "bush veto power he declared the <unk> <eos> <eos> <unk>\n",
            "bush veto power he declared the <unk> <eos> <eos> <unk>\n",
            "bush veto power would require the fed to take advantage\n",
            "bush veto power would require the fed to take advantage\n",
            "bush veto power would require the fed to take advantage\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require the fed to get a\n",
            "bush veto power would require such a technical crisis as\n",
            "bush veto power would require such a technical crisis as\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n",
            "bush veto power would require such a technical crisis if\n"
          ]
        }
      ],
      "source": [
        "#@title Interpolation\n",
        "samples, text1, text2 = interpolate(model, 20,\"president\", 10)\n",
        "print(\"First sentence:\", text1)\n",
        "print(\"Second sentence:\", text2)\n",
        "\n",
        "for sample in samples: print(sample)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Play_lstm_seq_vae.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
